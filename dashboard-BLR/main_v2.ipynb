{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment used is nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use language tool for spell check\n",
    "1. Download the language-tool English file from https://www.languagetool.org/download/\n",
    "2. Place the zipped file in C:\\Users\\<\"username\">\\\\.cache\\language_tool_python\n",
    "3. Make the following changes in language-tool package codes. This could be present in C:\\Users\\sbanerje111521\\bin\\anaconda3-2022.10\\envs\\nlp\\Lib\\site-packages\\language_tool_python:\n",
    "    - In **download_lt.py**:\n",
    "        + Change `LATEST_VERSION = '5.7'` to `LATEST_VERSION = '5.6'`, or the version of the language-tool English file that you have.\n",
    "        + Comment `download_zip(language_tool_download_url, download_folder)` at around line 156. This is done to bypass the above steps that is already completed.\n",
    "    - In **which.py**:\n",
    "        + Define `JAVA_PATH = \"C:\\\\Users\\\\sbanerje111521\\\\bin\\\\jdk-18.0.2\\\\bin\"` in beginning (around line 12). This is to manually set JAVA_HOME path.\n",
    "        + Include `paths.append(JAVA_PATH)` statement in `get_path_list()` function definition (at around line 60). This is to include or append `JAVA_PATH` in path variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use `spacy` with English tokenizer `en_core_web_sm`\n",
    "1. Install `spacy` package using `pip install spacy` or `conda install spacy` in command prompt.\n",
    "2. Check the version of spacy version installed. To check version: `pip list` or `conda list` in command prompt.\n",
    "3. Download the same version `en_core_web_sm` model zip file from https://github.com/explosion/spacy-models/releases. Unzip it.\n",
    "4. Open your Python environment path. You can check by importing any package and typing `<package_name>.__path__` in Python\n",
    "5. Save the folder containing __init__.py file in the site-packages folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# all functions are stored\n",
    "from utilities import *\n",
    "\n",
    "# to detect tense\n",
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "\n",
    "# for grammatical check\n",
    "from language_tool_python import LanguageTool\n",
    "\n",
    "# to display progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_FULL_PATH = r\"C:\\Users\\sbanerje111521\\OneDrive - GROUP DIGITAL WORKPLACE\\Documents\\Soumya_docs\\python\\poc_final\\inputs\\BusinessDataDictionnary.xlsx\"\n",
    "INPUT_FULL_PATH = r\"C:\\Users\\sbanerje111521\\OneDrive - GROUP DIGITAL WORKPLACE\\Documents\\Soumya_docs\\python\\poc_final\\inputs\\Weekly_BMT_RISQ_6262023228AM.xlsx\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\sbanerje111521\\OneDrive - GROUP DIGITAL WORKPLACE\\Documents\\Soumya_docs\\python\\poc_final\\outputs\"\n",
    "\n",
    "SPECIAL_CHARS = ['$','%','&','*','@','#','!','`','~','^','€','¿']\n",
    "\n",
    "REMOVE_NOTE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(INPUT_FULL_PATH, sheet_name=\"AMER CDEs\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 rows missing Golden Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['CDE No'])\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CDE Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['CDE Status']=='Active']\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the text\n",
    "df['lower_def'] = df['CDE Definition'].progress_apply(lambda x: x.lower())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38986006-bbe5-4224-8daa-ec3b7515e752",
   "metadata": {},
   "source": [
    "### Remove new line character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593adc3-8d29-4959-893d-162103019301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_line_def'] = df['lower_def'].progress_apply(lambda x: remove_new_line(x, REMOVE_NOTE))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97516fab-aedd-4c13-8c76-0d25c3fd7d5e",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f5d6e-b449-4f23-bfa0-e52edecb204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the puntuation free text\n",
    "df['clean_def']= df['new_line_def'].progress_apply(lambda x:remove_punctuation(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af497c-00ea-4366-8eb1-aa6e7c2d78de",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfdcee-58e6-4729-9a3d-71e3d0132cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the function\n",
    "df['no_stopwords']= df['clean_def'].progress_apply(lambda x: remove_stopwords(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "df['lemmatized']= df['no_stopwords'].progress_apply(lambda x: lemmatizer(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lemmatized_archer_result.pkl', 'wb') as wfile:\n",
    "    pickle.dump(df, wfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"lemmatized_archer_result.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_of_words'] = df['new_line_def'].progress_apply(lambda x: count_words(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count no. of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_of_chars'] = df['new_line_def'].progress_apply(lambda x: count_chars(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['list_spl_chars', 'count_spl_chars']] = df['new_line_def'].progress_apply(lambda x: count_spl_chars(x,SPECIAL_CHARS))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. of Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['list_acronym', 'count_acronym']] = df['CDE Definition'].progress_apply(lambda x: count_acronym(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To detect Present Tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_past_tense'] = df['clean_def'].progress_apply(lambda x: detect_past_sentece(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_present_tense'] = df['clean_def'].progress_apply(lambda x: detect_tense(x, 'present'))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check proper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['not_starting_upper_case', 'count_not_starting_uppercase', 'uppercase_in_between', 'count_uppercase_between']] = df['CDE Definition'].progress_apply(lambda x: check_propercase(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammatical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = LanguageTool('en-US')\n",
    "\n",
    "#df[['Mistakes','Corrections']] = df['clean_def'].progress_apply(lambda x: language_check(x, tool))\n",
    "df[['Mistakes','count_mistakes','Corrections']] = df['CDE Definition'].progress_apply(lambda x: language_check(x, tool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('grammar_check_archer_result.pkl', 'wb') as wfile:\n",
    "    pickle.dump(df, wfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"grammar_check_archer_result.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer [new method](#new_similarity)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = None\n",
    "df['test_term'] = None\n",
    "df['test_def'] = None\n",
    "df['test_id'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "cols.extend(['test_id','score','test_term','test_def'])\n",
    "result = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "\n",
    "    test_id = df.loc[i, 'CDE No']\n",
    "    test_term = df.loc[i, 'CDE Name']\n",
    "    test_def = df.loc[i, 'CDE Definition']\n",
    "    test_lemmatized = df.loc[i, 'lemmatized']\n",
    "\n",
    "    df2['score'] = df['lemmatized'].apply(lambda x: jaccard_similarity(x, test_lemmatized))\n",
    "    df2['test_term'] = test_term\n",
    "    df2['test_def'] = test_def\n",
    "    df2['test_id'] = test_id\n",
    "\n",
    "    df_ = df2.sort_values('score', ascending=False)\n",
    "    df_.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # store values\n",
    "    df.loc[i, 'score'] = df_.loc[1, 'score']\n",
    "    df.loc[i, 'test_term'] = df_.loc[1, 'CDE Name']\n",
    "    df.loc[i, 'test_def'] = df_.loc[1, 'CDE Definition']\n",
    "    df.loc[i, 'test_id'] = df_.loc[1, 'CDE No']\n",
    "\n",
    "    # if i in (114, 234):\n",
    "    #     store_df = df.copy()\n",
    "    #     store_df2 = df2.copy()\n",
    "\n",
    "    # if i==0:\n",
    "    #     result = df.iloc[[1],:]\n",
    "    # else:\n",
    "    #     result = pd.concat([result, df.iloc[[1],:]], axis=0, ignore_index=True)\n",
    "\n",
    "df[['CDE No','CDE Name','CDE Definition','score','test_term','test_def','test_id']].head(3)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pickle\n",
    "file = open(\"grammar_check_archer_result.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['score'] = None\n",
    "# df['test_term'] = None\n",
    "# df['test_def'] = None\n",
    "# df['test_id'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cols = ['CDE No','CDE Name','CDE Definition']\n",
    "result_cols = df.columns.to_list()\n",
    "# result_df = pd.DataFrame(columns=df.columns.to_list())\n",
    "result_df = pd.DataFrame(columns=result_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "\n",
    "    test_id = df.loc[i, 'CDE No']\n",
    "    # test_term = df.loc[i, 'CDE Name']\n",
    "    # test_def = df.loc[i, 'CDE Definition']\n",
    "    test_lemmatized = df.loc[i, 'lemmatized']\n",
    "\n",
    "    dup_df = df.copy()\n",
    "    dup_df['score'] = df['lemmatized'].apply(lambda x: jaccard_similarity(x, test_lemmatized))\n",
    "\n",
    "    # drop same record\n",
    "    dup_df.drop(dup_df[dup_df['CDE No']==test_id].index, inplace=True)\n",
    "\n",
    "    # descending order\n",
    "    dup_df = dup_df.sort_values('score', ascending=False)\n",
    "    dup_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # assign values\n",
    "    # for col in df.columns:\n",
    "    for col in result_cols:\n",
    "        result_df.loc[i, col] = df.loc[i, col]\n",
    "    \n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "    result_df.loc[i, 'score'] = dup_df.loc[0, 'score']\n",
    "    result_df.loc[i, 'test_id'] = dup_df.loc[0, 'CDE No']\n",
    "    result_df.loc[i, 'test_term'] = dup_df.loc[0, 'CDE Name']\n",
    "    result_df.loc[i, 'test_def'] = dup_df.loc[0, 'CDE Definition']\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('final_archer_top1_result.pkl', 'wb') as wfile:\n",
    "    pickle.dump(result_df, wfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top N similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"grammar_check_archer_result.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "df_['score'] = None\n",
    "df_['test_term'] = None\n",
    "df_['test_def'] = None\n",
    "df_['test_id'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_.copy()\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_.columns)\n",
    "cols.extend(['test_id','score','test_term','test_def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=cols)\n",
    "#temp = result.copy()\n",
    "\n",
    "for i in tqdm(range(len(df_))):\n",
    "\n",
    "    temp = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    test_id = df_.loc[i, 'CDE No']\n",
    "    test_term = df_.loc[i, 'CDE Name']\n",
    "    test_def = df_.loc[i, 'CDE Definition']\n",
    "    test_lemmatized = df_.loc[i, 'lemmatized']\n",
    "\n",
    "    df2['score'] = df_['lemmatized'].apply(lambda x: jaccard_similarity(x, test_lemmatized))\n",
    "    df2['test_term'] = test_term\n",
    "    df2['test_def'] = test_def\n",
    "    df2['test_id'] = test_id\n",
    "\n",
    "    df2 = df2.sort_values('score', ascending=False)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # store values\n",
    "    # df.loc[i, 'score'] = df2.loc[1, 'score']\n",
    "    # df.loc[i, 'test_term'] = df2.loc[1, 'test_term']\n",
    "    # df.loc[i, 'test_def'] = df2.loc[1, 'test_def']\n",
    "\n",
    "    # store values\n",
    "    for n in range(TOP_N):\n",
    "        for col in df2.columns:\n",
    "            temp.loc[n, col] = df2.loc[n+1, col]\n",
    "\n",
    "    result = pd.concat([result, temp], axis=0, ignore_index=True)\n",
    "\n",
    "    # if i==0:\n",
    "    #     result = df.iloc[[1],:]\n",
    "    # else:\n",
    "    #     result = pd.concat([result, df.iloc[[1],:]], axis=0, ignore_index=True)\n",
    "\n",
    "result.head(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[['CDE No','CDE Name','CDE Definition','score','test_term','test_def','test_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('final_archer_result_topN.pkl', 'wb') as wfile:\n",
    "    pickle.dump(result, wfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='new_similarity'>New method</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"grammar_check_archer_result.pkl\",'rb')\n",
    "df = pickle.load(file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 3\n",
    "# TOP_N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cols = ['CDE No','CDE Name','CDE Definition']\n",
    "result_cols = df.columns.to_list()\n",
    "# result_df = pd.DataFrame(columns=df.columns.to_list())\n",
    "result_df = pd.DataFrame(columns=result_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_row=0\n",
    "for i in range(len(df)):\n",
    "\n",
    "    test_id = df.loc[i, 'CDE No']\n",
    "    test_term = df.loc[i, 'CDE Name']\n",
    "    test_def = df.loc[i, 'CDE Definition']\n",
    "    test_lemmatized = df.loc[i, 'lemmatized']\n",
    "\n",
    "    dup_df = df.copy()\n",
    "    dup_df['score'] = df['lemmatized'].apply(lambda x: jaccard_similarity(x, test_lemmatized))\n",
    "\n",
    "    # drop same record\n",
    "    dup_df.drop(dup_df[dup_df['CDE No']==test_id].index, inplace=True)\n",
    "\n",
    "    # descending order\n",
    "    dup_df = dup_df.sort_values('score', ascending=False)\n",
    "    dup_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # assign values\n",
    "    # result_row = len(result_df)+1\n",
    "    # for col in result_cols:\n",
    "    #     for row in range(TOP_N):\n",
    "    #         result_df.loc[result_row, col] = df.loc[i, col]\n",
    "    #     result_row+=1\n",
    "\n",
    "    temp_df = pd.DataFrame(columns=result_cols)\n",
    "    append_list = [temp_df]\n",
    "    for _ in range(TOP_N):\n",
    "        append_list.append(df.loc[i,result_cols].to_frame().T)\n",
    "    temp_df = pd.concat(append_list, ignore_index=True)\n",
    "    # temp_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for row in range(TOP_N):\n",
    "        temp_df.loc[row, 'score'] = dup_df.loc[row, 'score']\n",
    "        temp_df.loc[row, 'test_id'] = dup_df.loc[row, 'CDE No']\n",
    "        temp_df.loc[row, 'test_term'] = dup_df.loc[row, 'CDE Name']\n",
    "        temp_df.loc[row, 'test_def'] = dup_df.loc[row, 'CDE Definition']\n",
    "\n",
    "    result_df = pd.concat([result_df, temp_df], axis=0)\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # result_df.reset_index(drop=True, inplace=True)\n",
    "    # for row in range(TOP_N):\n",
    "    #     result_df.loc[result_row - (TOP_N+1) + row, 'score'] = dup_df.loc[row, 'score']\n",
    "    #     result_df.loc[result_row - (TOP_N+1) + row, 'test_id'] = dup_df.loc[row, 'CDE No']\n",
    "    #     result_df.loc[result_row - (TOP_N+1) + row, 'test_term'] = dup_df.loc[row, 'CDE Name']\n",
    "    #     result_df.loc[result_row - (TOP_N+1) + row, 'test_def'] = dup_df.loc[row, 'CDE Definition']\n",
    "    # result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = current_time()\n",
    "# df.to_excel(OUTPUT_PATH+'\\\\'+f'final_result_{VERSION}.xlsx', index=False)\n",
    "# df.to_excel(OUTPUT_PATH+'\\\\'+f'final_archer_result_{VERSION}.xlsx', index=False)\n",
    "result_df.to_excel(OUTPUT_PATH+'\\\\'+f'final_archer_top1_result_{VERSION}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = current_time()\n",
    "result_df.to_excel(OUTPUT_PATH+'\\\\'+f'final_archer_result_topN_{VERSION}.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for lem in tqdm(df[\"lemmatized\"].values):\n",
    "    \n",
    "    for token in lem:\n",
    "        if token not in word_count:\n",
    "            word_count[token] = 1\n",
    "        else:\n",
    "            word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(word_count.keys())[:20]\n",
    "plt.figure(figsize=(10,6))\n",
    "ax1 = plt.subplot()\n",
    "ax1.set_xticklabels(labels, rotation=45)\n",
    "plt.bar(list(word_count.keys())[:20], list(word_count.values())[:20])\n",
    "plt.title(\"Top 20 terms\")\n",
    "plt.xlabel(\"Terms\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(tokens, n):\n",
    "    # tokens = vals.split()\n",
    "    n_gram = [tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "    return [' '.join(token) for token in n_gram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram= df['lemmatized'].progress_apply(lambda x: n_grams(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "for gram in tqdm(two_gram.values):\n",
    "    \n",
    "    for token in gram:\n",
    "        if token not in word_count:\n",
    "            word_count[token] = 1\n",
    "        else:\n",
    "            word_count[token] += 1\n",
    "\n",
    "word_count = dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(word_count.keys())[:10]\n",
    "plt.figure(figsize=(10,6))\n",
    "ax1 = plt.subplot()\n",
    "ax1.set_xticklabels(labels, rotation=45)\n",
    "plt.title(\"Top 10 terms\")\n",
    "plt.xlabel(\"Terms\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.bar(list(word_count.keys())[:10], list(word_count.values())[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d602ae7d171cf85674edbb70fa460d1d608bc628cef5b8fbe810ea97a75496a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
